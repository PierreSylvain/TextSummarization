{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load librairies \n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_from_file(filename):\n",
    "    \"\"\"Returns a list of sentences extracted from a file given as argument\n",
    "    @params\n",
    "        filename name of the file to load\n",
    "    @return\n",
    "        list sentences\n",
    "    \"\"\"    \n",
    "    sentences = []\n",
    "    lines = open(filename, \"r\")\n",
    "    for line in lines:\n",
    "       \n",
    "        \n",
    "        # Delete space at first and at the end of the sentence (if any)\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Don't take care of empty line\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Special case of ...    \n",
    "        line = line.replace(\"...\", \".\")\n",
    "        \n",
    "        # Split line with end of sentence sign . ! ? or CR or CR/LF        \n",
    "        line = re.split(r'[.!?]',line)\n",
    "        \n",
    "        # Append to list\n",
    "        for sentence in line:\n",
    "            if not sentence:\n",
    "                continue\n",
    "            sentences.append(sentence)\n",
    "            \n",
    "    lines.close()\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove non pertinent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_sentences(sentences):\n",
    "    \"\"\"Returns a list of words into a list of sentences \n",
    "       where punctation, stop word and more are removed\n",
    "    @params\n",
    "        sentences List of sentences\n",
    "    @return\n",
    "        list of filtered sentences\n",
    "    \n",
    "    \"\"\"\n",
    "    filtered_sentences = []\n",
    "    \n",
    "    # Only french stop words\n",
    "    stop_words = stopwords.words('french')\n",
    "    \n",
    "    for sentence in sentences:            \n",
    "        if not sentence:\n",
    "            continue\n",
    "            \n",
    "        # Sentence in lower case\n",
    "        sentence = sentence.lower()\n",
    "            \n",
    "        # Remove some unwanted characters like punctuation and \"\n",
    "        sentence = sentence.replace(\"«\", \"\")\n",
    "        sentence = sentence.replace(\"»\", \"\")\n",
    "        sentence = sentence.replace(\",\", \"\")\n",
    "        sentence = sentence.replace(\";\", \"\")\n",
    "        sentence = sentence.replace(\":\", \"\")\n",
    "        sentence = sentence.replace(\"\\\"\", \"\")\n",
    "        sentence = sentence.replace(\"(\", \"\")\n",
    "        sentence = sentence.replace(\")\", \"\")            \n",
    "            \n",
    "        # Replace contracted forms            \n",
    "        sentence = sentence.replace(\"n'\", \"ne \")\n",
    "        sentence = sentence.replace(\"n’\", \"ne \")\n",
    "        sentence = sentence.replace(\"l'\", \"le \")\n",
    "        sentence = sentence.replace(\"d'\", \"de \")\n",
    "        sentence = sentence.replace(\"d’\", \"de \")        \n",
    "        sentence = sentence.replace(\"s'\", \"si \")\n",
    "        sentence = sentence.replace(\"qu'\", \"que \")\n",
    "        sentence = sentence.replace(\"c'\", \"ce \")\n",
    "            \n",
    "        # Remove space\n",
    "        sentence = sentence.strip()\n",
    "                        \n",
    "        # Split in words and remove stopwords\n",
    "        filtered_words = [word for word in sentence.split(\" \") if word not in stopwords.words('french')]\n",
    "        filtered_sentences.append(filtered_words)\n",
    "        \n",
    "    return filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le constat d'échec de la justice dans la prévention des homicides conjugaux\n",
      "[['constat', 'échec', 'justice', 'prévention', 'homicides', 'conjugaux'], ['rapport', 'inspection', 'générale', 'justice', 'homicides', 'conjugaux', '88', 'cas', 'définitivement', 'jugés', 'pointe', 'graves', 'dysfonctionnements', 'chaîne', 'pénale'], ['décidant', 'rendre', 'public', 'dimanche', '17', 'novembre', 'rapport', 'inspection', 'générale', 'justice', 'homicides', 'conjugaux', 'nicole', 'belloubet', 'garde', 'sceaux', 'dévoile', 'sans', 'fard', 'cloche', 'détection', 'signes', 'annonciateurs', 'crimes'], ['constat', 'alarmant', 'tant', 'côté', 'services', 'police', 'gendarmerie', 'côté', 'magistrats', 'services', 'pénitentiaires', 'services', 'sociaux', 'médicaux'], ['très', 'clairement', 'ça', 'va'], ['chaîne', 'pénale', 'satisfaisante', 'reconnaît', 'ministre', 'justice', 'entretien', 'publié', 'jour', 'journal', 'dimanche'], ['mission', 'inspection', 'a', 'examiné', '88', 'dossiers', 'homicides', 'conjugaux', 'tentatives', 'homicides', 'violences', 'volontaires', 'entraîné', 'mort', 'sans', 'intention', 'donner', 'commis', '2015', '2016', 'définitivement', 'jugés', 'depuis'], ['73', 'victimes', 'femmes', '15', 'hommes'], ['traitement', 'judiciaire', 'crime', 'fois', 'commis', 'semble', 'plutôt', 'satisfaisant'], ['durée', 'moyenne', 'instruction', 'judiciaire', '17', 'mois', 'contre', '31', 'mois', 'crimes', 'moyenne', 'sanctions', 'prononcées', 'cours', 'assises', 'traduisent', 'prise', 'conscience', 'particularité', 'crimes'], ['durée', 'moyenne', 'réclusion', 'criminelle', '17', 'ans', 'lit-on', 'rapport', 'au-dessus', 'moyenne', 'condamnations', 'meurtres', 'hors', 'contexte', 'conjugal'], ['surtout', 'peines', 'homicides', 'conjugaux', 'plus', 'plus', 'lourdes'], ['elles', 'treize', 'ans', 'réclusion', '2004'], ['amont', 'meurtre', 'justice', 'clairement', 'hauteur'], ['car', '63%', 'cas', 'violences', 'antérieures', 'existaient', 'certes', 'toujours', 'dénoncées', 'forces', 'ordre'], ['35%', 'cas', 'où', 'violences', 'préexistaient', 'elles', 'dénoncées', 'police', 'plus', 'souvent', 'connues', 'famille', 'voisins', 'services', 'sociaux'], ['cette', 'absence', 'dénonciation', 'signalement', 'a', 'empêché', 'mise', 'place', 'mesures', 'susceptibles', 'prévenir', 'homicide', 'ultérieur', 'note', 'l’inspection'], ['absence', 'dénonciation', 'médecins', 'également', 'déplorée', 'alors', 'dizaine', 'victimes', 'violences', 'conjugales', 'auparavant', 'consulté', 'hôpital', 'cabinet'], ['rapport', 'relate', 'ainsi', 'cas', 'victime', 'si', 'elle-même', 'rendue', 'dix', 'reprises', 'urgences', 'entre', '2005', '2014', 'dont', 'quatre', 'fois', 'année', '', 'avant', 'être', 'tuée', 'arme', 'feu', 'conjoint']]\n"
     ]
    }
   ],
   "source": [
    "sentences = get_sentences_from_file( \"data/article_002.txt\")\n",
    "print(sentences[0])\n",
    "filtered_sentences = get_filtered_sentences(sentences)\n",
    "print(filtered_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarity(a, b):\n",
    "    \"\"\"Returns the cosine similarity of 2 vectors\n",
    "    @params\n",
    "        a vector\n",
    "        b vector\n",
    "    @return\n",
    "        cosine similarity\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vector(filtered_sentences):\n",
    "    \"\"\"Returns vectorized sentences with the TF-IDF of the words\n",
    "    @params:\n",
    "        filetred_sentences list of sentences with list of words\n",
    "    @return:\n",
    "        List of the same sentences where the words are replaced by \n",
    "          the TF-IDF of the word\n",
    "    \"\"\"\n",
    "    term_frequency = []\n",
    "    sentence_max_len = 0\n",
    "    \n",
    "    # Get the occurence of each word in a sentence\n",
    "    for sentence in filtered_sentences:\n",
    "        if sentence_max_len < len(sentence):\n",
    "            sentence_max_len = len(sentence)\n",
    "\n",
    "        words = {}\n",
    "        for word in sentence:\n",
    "            if word in words:\n",
    "                words[word] += 1\n",
    "            else:\n",
    "                words[word] = 1\n",
    "                        \n",
    "        term_frequency.append(words)\n",
    "\n",
    "    # Term Frequency \n",
    "    # word occurence / total count of word in the sentence    \n",
    "    # Prepare IDF\n",
    "    # Occurence of the words in sentences\n",
    "    for terms in term_frequency:\n",
    "        terms_count = len(terms)\n",
    "        for term in terms:\n",
    "            terms[term] = terms[term] / terms_count    \n",
    "            if term in words_idf:\n",
    "                words_idf[term] += 1\n",
    "            else:\n",
    "                words_idf[term] = 1\n",
    "        \n",
    "    # IDF\n",
    "    document_count = len(filtered_sentences)\n",
    "    for word_idf in words_idf:\n",
    "        words_idf[word_idf] = math.log(document_count / words_idf[word_idf])\n",
    "\n",
    "    # TF-IDF for each word and vectorize sentences\n",
    "    tf_idfs = []\n",
    "\n",
    "    for idx, sentence in enumerate(filtered_sentences):\n",
    "        tf_idf = []\n",
    "        # The vectors must have the same length\n",
    "        vector = [0] * sentence_max_len\n",
    "        for word_index, word in enumerate(sentence):\n",
    "            tfidf = term_frequency[idx][word] * words_idf[word]\n",
    "            vector[word_index] = tfidf\n",
    "        tf_idfs.append(vector)\n",
    "    return tf_idfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(filtered_sentences):\n",
    "    \"\"\"Returns vectorized sentences with the index of the words\n",
    "    @params:\n",
    "        filetred_sentences list of sentences with list of words\n",
    "    @return:\n",
    "        List of the same sentences where the words are replaced by \n",
    "          the word ID\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    sentence_max_len = 0\n",
    "    words = {}\n",
    "    idx = 0\n",
    "    for sentence in filtered_sentences:\n",
    "        if sentence_max_len < len(sentence):\n",
    "            sentence_max_len = len(sentence)\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word not in words:\n",
    "                words[word] = idx\n",
    "                idx += 1\n",
    "    \n",
    "    for idx, sentence in enumerate(filtered_sentences):\n",
    "        vector = [0] * sentence_max_len\n",
    "        for word_index, word in enumerate(sentence):\n",
    "            vector[word_index] = words[word]\n",
    "        vectors.append(vector)\n",
    "        \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcount_vector(filtered_sentences):\n",
    "    \"\"\"Returns vectorized sentences with the count of words\n",
    "    @params:\n",
    "        filetred_sentences list of sentences with list of words\n",
    "    @return:\n",
    "        List of the same sentences where the words are replaced by \n",
    "          the count of the word\n",
    "    \"\"\"\n",
    "    term_frequency = {}\n",
    "    sentence_max_len = 0\n",
    "    \n",
    "    for sentence in filtered_sentences:\n",
    "        if sentence_max_len < len(sentence):\n",
    "            sentence_max_len = len(sentence)\n",
    "\n",
    "        for word in sentence:\n",
    "            if word in term_frequency:\n",
    "                term_frequency[word] += 1\n",
    "            else:\n",
    "                term_frequency[word] = 1                        \n",
    "\n",
    "    tf_idfs = []\n",
    "\n",
    "    for idx, sentence in enumerate(filtered_sentences):\n",
    "        tf_idf = []        \n",
    "        vector = [0] * sentence_max_len\n",
    "        for word_index, word in enumerate(sentence):            \n",
    "            vector[word_index] = term_frequency[word]\n",
    "        tf_idfs.append(vector)\n",
    "    return tf_idfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : Le constat d'échec de la justice dans la prévention des homicides conjugaux\n",
      "---------------\n",
      "L'absence de dénonciation par les médecins est également déplorée, alors qu'une dizaine de victimes de violences conjugales avaient auparavant consulté à l'hôpital ou en cabinet.\n",
      " \"Cette absence de dénonciation ou de signalement a empêché la mise en place de mesures susceptibles de prévenir l'homicide ultérieur\", note l’inspection.\n",
      " Dans 35% des cas où des violences préexistaient, elles n’avaient pas été dénoncées à la police, mais étaient le plus souvent connues de la famille, des voisins ou de services sociaux.\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "\n",
    "words_idf = {}\n",
    "\n",
    "# Load sentences\n",
    "sentences = get_sentences_from_file( \"data/article_002.txt\")\n",
    "\n",
    "# Clean sentences\n",
    "filtered_sentences = get_filtered_sentences(sentences)\n",
    "\n",
    "# Convert sentences toi vectors\n",
    "vectors = tfidf_vector(filtered_sentences)\n",
    "#vectors = word_vector(filtered_sentences)\n",
    "#vectors = wordcount_vector(filtered_sentences)\n",
    "    \n",
    "# Get the cosine similarity betwwen all the sentences\n",
    "similarity_matrix = np.zeros((len(vectors), len(vectors)))\n",
    "\n",
    "for idx1, sent1 in enumerate(vectors):\n",
    "    for idx2, sent2 in enumerate(vectors):\n",
    "        if idx1 == idx2:\n",
    "            continue\n",
    "        similarity_matrix[idx1][idx2] = get_cosine_similarity(np.array(sent1), np.array(sent2))\n",
    "\n",
    "# Rank sentences to get the best cosine similatity first\n",
    "rank = []     \n",
    "for idx, sim in enumerate(similarity_matrix):\n",
    "    rank.append((idx,sum(sim)))\n",
    "\n",
    "sorted_rank = sorted(rank, key=lambda sim: sim[1], reverse=True)\n",
    "top_n = 3\n",
    "\n",
    "# Print result\n",
    "print(f\"Title : {sentences[0]}\")\n",
    "print (\"---------------\")\n",
    "for r in sorted_rank:\n",
    "    idx = r[0]\n",
    "    print(sentences[idx] + '.')\n",
    "    top_n -= 1\n",
    "    if top_n == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
